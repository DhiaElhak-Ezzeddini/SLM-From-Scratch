{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946bf3129e8baedb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T21:05:38.001851Z",
     "start_time": "2025-09-20T21:04:55.945205Z"
    }
   },
   "outputs": [],
   "source": [
    "from SLM_Architecture import GPTModel,GPTConfig,Transformer_Block\n",
    "\n",
    "import importlib\n",
    "import Dataset_Preprocessing\n",
    "importlib.reload(Dataset_Preprocessing)\n",
    "from Dataset_Preprocessing import download_dataset, get_tokenizer,  Build_Dataset,process,get_batch\n",
    "\n",
    "import torch \n",
    "from contextlib import nullcontext\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim.lr_scheduler import LinearLR,SequentialLR,CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dfb5a4",
   "metadata": {},
   "source": [
    "## Download and build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5dd533",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = download_dataset()\n",
    "tokenizer = get_tokenizer(\"gpt2\")\n",
    "Build_Dataset(dataset,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89049e9ba93b7af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTConfig(\n",
    "    vocab_size=50257,\n",
    "    block_size=128,\n",
    "    n_layer=6,\n",
    "    n_head=6,\n",
    "    n_embed=384,\n",
    "    dropout=0.1,\n",
    "    bias=True\n",
    "    \n",
    ")\n",
    "model = GPTModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba9c078",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cec18b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    with torch.inference_mode():    \n",
    "        for split in [\"train\",\"eval\"] : \n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                X,Y = get_batch(split)\n",
    "                with ctx : \n",
    "                    logits,loss = model(X,Y)\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8897325",
   "metadata": {},
   "source": [
    "## Training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83af326",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr= 1e-4\n",
    "max_iters = 20000\n",
    "warmup_steps = 100\n",
    "min_lr = 5e-4\n",
    "eval_iters = 500\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "\n",
    "gradient_accumulation_steps = 32 \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_type = \"cuda\" if \"cuda\" == device else \"cpu\"\n",
    "dtype = \"bfloat16\" if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else \"float16\"\n",
    "ptdtype = {\"float32\":torch.float32,\"bfloat16\":torch.bfloat16,\"float1\":torch.float16}[dtype]\n",
    "\n",
    "ctx = nullcontext() if device == \"cpu\" else torch.amp.autocast(device_type=device_type,dtype=ptdtype)\n",
    "torch.set_default_device(device)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3712070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(),lr=lr,betas=(0.9,0.95),weight_decay=0.1,eps=1e-9)\n",
    "scheduler_warmup = LinearLR(optimizer,total_iters=warmup_steps)\n",
    "scheduler_decay = CosineAnnealingLR(optimizer,T_max=max_iters - warmup_steps,eta_min=min_lr)\n",
    "scheduler = SequentialLR(optimizer,schedulers=[scheduler_warmup,scheduler_decay],milestones=[warmup_steps])\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == \"float16\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3b8b35",
   "metadata": {},
   "source": [
    "## Pretrain SLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb6125",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_model_params_path = \"best_model_params.pt\"\n",
    "train_loss_list,val_loss_list = [],[]\n",
    "\n",
    "model = model.to(device)\n",
    "for epoch in tqdm(range(max_iters)) :\n",
    "    if epoch % eval_iters == 0 and epoch != 0 : \n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"Epoch {epoch}: train loss {losses[\"train\"]:.4f}, val loss {losses[\"val\"]:.4f}\")\n",
    "        print(f\"The current learning rate: {optimizer.param_groups[0][\"lr\"]:.5f}\") \n",
    "        train_loss_list += [losses[\"train\"]]\n",
    "        val_loss_list   += [losses[\"val\"]]\n",
    "        \n",
    "        if losses[\"val\"] < best_val_loss:\n",
    "            best_val_loss = losses[\"val\"]\n",
    "            torch.save(model.state_dict(),best_model_params_path)\n",
    "    X,y = get_batch(\"train\",block_size,batch_size,device=device,train_path=\"train.bin\",val_path=\"validation.bin\")\n",
    "    X,y = X.to(device),y.to(device)\n",
    "    with ctx : \n",
    "        logits , loss = model(X,y)\n",
    "        scaler.scale(loss).backward()\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "    if ((epoch+1) % gradient_accumulation_steps ==0) or (epoch+1 == max_iters) : \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=0.5)\n",
    "        scaler.step(optimizer)\n",
    "    scheduler.step()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d12d3fe",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
