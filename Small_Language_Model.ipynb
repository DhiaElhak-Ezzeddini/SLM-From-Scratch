{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946bf3129e8baedb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T21:05:38.001851Z",
     "start_time": "2025-09-20T21:04:55.945205Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np \n",
    "from tqdm.auto import tqdm \n",
    "from contextlib import nullcontext\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89049e9ba93b7af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,ndim,bias) : \n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.parameter(torch.zeros(ndim)) if bias else None\n",
    "    def forward(self,x):\n",
    "        return F.layer_norm(x,self.weight.shape,self.weight,self.bias,1e-5)\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        assert config.n_embed % config.n_head == 0 \n",
    "        self.c_attn = nn.Linear(config.n_embed,3*config.n_embed,bias=config.bias) ## for the 3 matrices ; K Q V\n",
    "        self.c_proj = nn.Linear(config.n_embed,config.n_embed,bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.residual_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embed = config.n_embed\n",
    "        self.flash = hasattr(F,\"scaled_dot_product_attention\")\n",
    "        if not self.flash : \n",
    "            self.register_buffer(\"bias\",torch.tril(torch.ones(config.block_size,config.block_size)).view(1,1,config.block_size,config.block_size))\n",
    "    \n",
    "    def forward(self,x) : \n",
    "        B,T,C = x.size() \n",
    "        q,k,v = self.c_attn(x).split(self.n_embed,dim=2)\n",
    "        q = q.view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n",
    "        k = k.view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n",
    "        v = v.view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n",
    "        \n",
    "        if self.flash : \n",
    "            y = F.scaled_dot_product_attention(q,k,v,attn_mask=None,dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n",
    "        else : \n",
    "            att = (q@k.transpose(-2,-1)) * (1.0/math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T]==0,float(\"-inf\"))\n",
    "            att = F.softmax(att,dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v \n",
    "        y = y.transpose(1,2).contiguous().view(B,T,C)\n",
    "        y = self.residual_dropout(self.c_proj(y))\n",
    "        \n",
    "        return y         \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,config) : \n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embed,4*config.n_embed,bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4*config.n_embed,config.n_embed,bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x)))) \n",
    "class Transformer_Block(nn.Module):\n",
    "    def __init__(self,config) : \n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(config.n_embed,config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln1 = LayerNorm(config.n_embed,config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        \n",
    "        return x \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
